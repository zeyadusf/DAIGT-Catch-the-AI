<div align="center">

# DAIGT

</div>

## ğŸ¤ About Files :

### 1ï¸âƒ£ First Term Version

```
  * DataSet : ( train_essays.csv ,train_drcat_02.csv ) on Train -- (train_drcat_04.csv) on Test.
  * EDA.
  * Model : from scratch architecture model
              - Bi-LSTM
              - CNN
```
Links :
- <b>[LLM - Detect AI Generated Text](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/data)</b>
- <b>[DAIGT Proper Train Dataset](https://www.kaggle.com/datasets/thedrcat/daigt-proper-train-dataset)</b>
on [Google Drive](https://drive.google.com/drive/folders/1MVqODc8gP812yRJD0h6yDsuA6r390JKK)

<hr>

### 2ï¸âƒ£ Second Term Versions :

#### ğŸ“ First Step:

```
> Data Augmentation :
  - from Hugging Face & Kaggle.
    + Balanced Data .
    + Diff. Dist.
> Data Preprocessing :
   - Deep Clean method.
   - Tokenization .
> Model Archtecture :
  - BERT as Embedding laye .
  - biuld model From scratch.
```
Links :<br>
[Model | DAIGT](https://www.kaggle.com/code/zeyadusf/model-daigt)<br>
[Create DataSet](https://www.kaggle.com/code/zeyadusf/create-dataset)<br>
[DAIGT | All data For Model.](https://www.kaggle.com/datasets/zeyadusf/daigt-all-data-for-competition)<br>


#### ğŸ“ Second Step:

```
> Data Preparation :
  - Use torch 'DataSet'.
  - Without Clean.
  - Tokenizers.
> Model Architecture:
  - Mistral-7B:
    + Flash attention.
    + Quantization in 4-bits.
    + LoRAdapter.
> Train Methods:
  - Train Loop from Scratch.
  - Trainer & Train Argu.
```
Links: <br>
ğŸŒµ First attempt  : [Model 2 | DAIGT](https://www.kaggle.com/code/zeyadusf/model-2-daigt)<br>
ğŸŒµ Second attempt : [Model 3 | DAIGT](https://www.kaggle.com/code/zeyadusf/model-3-daigt)<br>
ğŸŒµ Third attempt  : [Model 4 | DAIGT](https://www.kaggle.com/code/zeyadusf/model-4-daigt)<br>
ğŸŒµ Fourth attempt : [Attempt number 1000](https://www.kaggle.com/code/zeyadusf/attempt-number-1000)<br>
ğŸŒµ Fifth attempt  : [1000+1](https://www.kaggle.com/code/zeyadusf/1000-1)  -  'acc2'<br>
ğŸŒµ Sixth attempt  : [1000+2](https://www.kaggle.com/code/zeyadusf/1000-2)   - 'acc3'<br>
ğŸŒµ Seventh attempt: [RoBERTa-fine_we_Ana L2](https://www.kaggle.com/code/oknomore/roberta-fine-we-ana-l2) - 'acc2'<br>

```
- Save & Load MyMistral from hugging.
- Convert data to Batches.
- Train the model on each Fold.
```
Links:<br>
ğŸŒµEighth attempt : [take batch or take my life|DAIGT](https://www.kaggle.com/code/zeyadusf/take-batch-or-take-my-life-daigt)<br>
ğŸŒµ Ninth attempt : [test_2_batchTaker](https://www.kaggle.com/code/oknomore/test-2-batchtaker)

Code Create Folds :  [CreateFolds | DAIGT](https://www.kaggle.com/code/zeyadusf/createfolds-daigt)<br>
DataSet After Make it [Folds_102 | DAIGT](https://www.kaggle.com/code/zeyadusf/createfolds-daigt)<br>


#### ğŸ“ Third Step:

```
>  Usaged Data:
  - From Kaggle Onlyyy.
> Models Architectures:
  - Mistral-7B   ğŸ§Ÿâ€â™‚ï¸ Failed to Save and recall.
  - RoBERTa.
  - DistilBERT.
  - DeBERTa.
  - BERT.
```
Links:<br>
[RoBERTa](https://www.kaggle.com/code/abdelrahman020/roberta)<br>
[DistilBERT](https://www.kaggle.com/code/oknomore/distilbert)<br>
[BBERT](https://www.kaggle.com/code/oknomore/bbert)<br>
[DeBERTa-V2](https://www.kaggle.com/code/abdelrahman020/debert-v2)<br>


#### ğŸ“ fourth Step:
```
ğŸš§ Announcement will be made after completion ....
```



7 may.
